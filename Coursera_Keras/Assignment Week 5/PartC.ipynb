{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"concrete_data.csv\")\n",
    "df_y = df[\"Strength\"]\n",
    "df_X = df.drop([\"Strength\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C. Use 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a single hidden layer of 10 nodes, and a ReLU activation function\n",
    "model.add(Dense(10, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))  # Output layer for linear regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using Adam optimizer and loss mean_squared_error\n",
    "model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column, calculate mean and standard deviation and then for each value, subtract mean and divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>-0.045623</td>\n",
       "      <td>0.487998</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>-0.092126</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>-1.322363</td>\n",
       "      <td>-0.065861</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.392628</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.959602</td>\n",
       "      <td>0.675872</td>\n",
       "      <td>0.702285</td>\n",
       "      <td>-1.993711</td>\n",
       "      <td>0.496651</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>-1.269472</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.850222</td>\n",
       "      <td>0.521336</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-1.035561</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-1.168042</td>\n",
       "      <td>1.307430</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>0.214537</td>\n",
       "      <td>0.191074</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-0.193939</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.376762</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.400971</td>\n",
       "      <td>-1.394385</td>\n",
       "      <td>-0.150675</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4    -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "...        ...                 ...       ...       ...               ...   \n",
       "1025 -0.045623            0.487998  0.564271 -0.092126          0.451190   \n",
       "1026  0.392628           -0.856472  0.959602  0.675872          0.702285   \n",
       "1027 -1.269472            0.759210  0.850222  0.521336         -0.017520   \n",
       "1028 -1.168042            1.307430 -0.846733 -0.279443          0.852942   \n",
       "1029 -0.193939            0.308349  0.376762  0.891286          0.400971   \n",
       "\n",
       "      Coarse Aggregate  Fine Aggregate       Age  \n",
       "0             0.862735       -1.217079 -0.279597  \n",
       "1             1.055651       -1.217079 -0.279597  \n",
       "2            -0.526262       -2.239829  3.551340  \n",
       "3            -0.526262       -2.239829  5.055221  \n",
       "4             0.070492        0.647569  4.976069  \n",
       "...                ...             ...       ...  \n",
       "1025         -1.322363       -0.065861 -0.279597  \n",
       "1026         -1.993711        0.496651 -0.279597  \n",
       "1027         -1.035561        0.080068 -0.279597  \n",
       "1028          0.214537        0.191074 -0.279597  \n",
       "1029         -1.394385       -0.150675 -0.279597  \n",
       "\n",
       "[1030 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df_X.columns:\n",
    "    # Calculate mean and standard deviation of each column\n",
    "    mean = df_X[column].mean()\n",
    "    std = df_X[column].std()\n",
    "    # Normalize the data\n",
    "    df_X[column] = df_X[column].apply(lambda x : (x - mean) / std)\n",
    "    \n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using the code in PART A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split helper function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model on the training data using 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 0s 838us/step - loss: 63.8301\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 63.1277\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 62.8687\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 62.5533\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 61.8495\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 60.8375\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 906us/step - loss: 59.4409\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 58.3130\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 56.6958\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 55.3959\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 54.6615\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 54.4159\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 53.6881\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 54.0913\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 53.2136\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 52.5653\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 52.3836\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 885us/step - loss: 52.4656\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.8266\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 51.0002\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 839us/step - loss: 50.8854\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 50.6505\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 50.1473\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 49.0141\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 47.7805\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 47.7270\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 46.9555\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 47.7860\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 46.8062\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 46.5012\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 46.4250\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 45.9666\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 46.9982\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 45.4674\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 942us/step - loss: 45.2730\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 45.5276\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 45.2473\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 45.0267\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.7812\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.5771\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.8861\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.6502\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.3886\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.2922\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.2789\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9867\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3966\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4179\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9241\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6463\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 43.3930\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 44.0922\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 43.0527\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 43.1525\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.9200\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 43.0451\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.6734\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.8916\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.3503\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.7072\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5092\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4518\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3297\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.0891\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2771\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.1971\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.1633\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.6881\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.8603\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.2111\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.3457\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.6096\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.2016\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.8856\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.3522\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6816\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.6196\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.0071\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.6395\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.6789\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6288\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2159\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8895\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8056\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1765\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.5485\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.3791\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.6471\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.6306\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.3056\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.4222\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 42.7464\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.8654\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0431\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.2947\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.2691\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.2640\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4108\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 41.6567\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7273\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 999us/step - loss: 41.8199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41.8199348449707"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer : Acceptable loss on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 43.3697\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 48.4129\n",
      "10/10 [==============================] - 0s 766us/step - loss: 43.1321\n",
      "10/10 [==============================] - 0s 732us/step - loss: 38.3176\n",
      "10/10 [==============================] - 0s 742us/step - loss: 36.6751\n",
      "10/10 [==============================] - 0s 731us/step - loss: 41.0717\n",
      "10/10 [==============================] - 0s 944us/step - loss: 43.8693\n",
      "10/10 [==============================] - 0s 749us/step - loss: 42.1564\n",
      "10/10 [==============================] - 0s 813us/step - loss: 36.3790\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 38.8201\n",
      "10/10 [==============================] - 0s 814us/step - loss: 41.9356\n",
      "10/10 [==============================] - 0s 905us/step - loss: 40.5504\n",
      "10/10 [==============================] - 0s 782us/step - loss: 38.2689\n",
      "10/10 [==============================] - 0s 716us/step - loss: 40.5067\n",
      "10/10 [==============================] - 0s 820us/step - loss: 46.9045\n",
      "10/10 [==============================] - 0s 737us/step - loss: 42.9315\n",
      "10/10 [==============================] - 0s 841us/step - loss: 39.4420\n",
      "10/10 [==============================] - 0s 757us/step - loss: 43.0712\n",
      "10/10 [==============================] - 0s 796us/step - loss: 39.0880\n",
      "10/10 [==============================] - 0s 869us/step - loss: 37.8186\n",
      "10/10 [==============================] - 0s 777us/step - loss: 37.2323\n",
      "10/10 [==============================] - 0s 703us/step - loss: 40.1061\n",
      "10/10 [==============================] - 0s 715us/step - loss: 39.5454\n",
      "10/10 [==============================] - 0s 856us/step - loss: 33.1171\n",
      "10/10 [==============================] - 0s 813us/step - loss: 42.0543\n",
      "10/10 [==============================] - 0s 727us/step - loss: 27.7528\n",
      "10/10 [==============================] - 0s 695us/step - loss: 32.0367\n",
      "10/10 [==============================] - 0s 812us/step - loss: 29.1265\n",
      "10/10 [==============================] - 0s 701us/step - loss: 35.9570\n",
      "10/10 [==============================] - 0s 907us/step - loss: 40.5198\n",
      "10/10 [==============================] - 0s 735us/step - loss: 40.7980\n",
      "10/10 [==============================] - 0s 751us/step - loss: 34.4835\n",
      "10/10 [==============================] - 0s 726us/step - loss: 33.2857\n",
      "10/10 [==============================] - 0s 774us/step - loss: 34.9152\n",
      "10/10 [==============================] - 0s 846us/step - loss: 28.9887\n",
      "10/10 [==============================] - 0s 820us/step - loss: 35.0908\n",
      "10/10 [==============================] - 0s 793us/step - loss: 32.1763\n",
      "10/10 [==============================] - 0s 785us/step - loss: 34.0435\n",
      "10/10 [==============================] - 0s 915us/step - loss: 35.2620\n",
      "10/10 [==============================] - 0s 790us/step - loss: 31.4459\n",
      "10/10 [==============================] - 0s 752us/step - loss: 38.0213\n",
      "10/10 [==============================] - 0s 823us/step - loss: 31.7700\n",
      "10/10 [==============================] - 0s 822us/step - loss: 34.2072\n",
      "10/10 [==============================] - 0s 831us/step - loss: 34.2740\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 30.2292\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29.2882\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 32.7731\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 32.3482\n",
      "10/10 [==============================] - 0s 870us/step - loss: 33.3726\n",
      "10/10 [==============================] - 0s 741us/step - loss: 32.6295\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_list =  [] # List to hold all the mean_squared_erros\n",
    "\n",
    "for step_number in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3)\n",
    "    history = model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    step_mean_squared_error = model.evaluate(X_test, y_test)\n",
    "    mean_squared_error_list.append(step_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared errors is :  36.99144165039063\n",
      "Standard Deviation of mean squared errors is :  23.441158190112027\n"
     ]
    }
   ],
   "source": [
    "mean_of_mse = sum(mean_squared_error_list) / len(mean_squared_error_list)\n",
    "standard_deviation_of_mse = sum([((x - mean_of_mse) ** 2) for x in mean_squared_error_list]) / len(mean_squared_error_list)\n",
    "\n",
    "print(\"Mean of mean squared errors is : \",mean_of_mse)\n",
    "print(\"Standard Deviation of mean squared errors is : \", standard_deviation_of_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
