{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"concrete_data.csv\")\n",
    "df_y = df[\"Strength\"]\n",
    "df_X = df.drop([\"Strength\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a single hidden layer of 10 nodes, and a ReLU activation function\n",
    "model.add(Dense(10, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))  # Output layer for linear regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using Adam optimizer and loss mean_squared_error\n",
    "model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column, calculate mean and standard deviation and then for each value, subtract mean and divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>-0.045623</td>\n",
       "      <td>0.487998</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>-0.092126</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>-1.322363</td>\n",
       "      <td>-0.065861</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.392628</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.959602</td>\n",
       "      <td>0.675872</td>\n",
       "      <td>0.702285</td>\n",
       "      <td>-1.993711</td>\n",
       "      <td>0.496651</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>-1.269472</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.850222</td>\n",
       "      <td>0.521336</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-1.035561</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-1.168042</td>\n",
       "      <td>1.307430</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>0.214537</td>\n",
       "      <td>0.191074</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-0.193939</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.376762</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.400971</td>\n",
       "      <td>-1.394385</td>\n",
       "      <td>-0.150675</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4    -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "...        ...                 ...       ...       ...               ...   \n",
       "1025 -0.045623            0.487998  0.564271 -0.092126          0.451190   \n",
       "1026  0.392628           -0.856472  0.959602  0.675872          0.702285   \n",
       "1027 -1.269472            0.759210  0.850222  0.521336         -0.017520   \n",
       "1028 -1.168042            1.307430 -0.846733 -0.279443          0.852942   \n",
       "1029 -0.193939            0.308349  0.376762  0.891286          0.400971   \n",
       "\n",
       "      Coarse Aggregate  Fine Aggregate       Age  \n",
       "0             0.862735       -1.217079 -0.279597  \n",
       "1             1.055651       -1.217079 -0.279597  \n",
       "2            -0.526262       -2.239829  3.551340  \n",
       "3            -0.526262       -2.239829  5.055221  \n",
       "4             0.070492        0.647569  4.976069  \n",
       "...                ...             ...       ...  \n",
       "1025         -1.322363       -0.065861 -0.279597  \n",
       "1026         -1.993711        0.496651 -0.279597  \n",
       "1027         -1.035561        0.080068 -0.279597  \n",
       "1028          0.214537        0.191074 -0.279597  \n",
       "1029         -1.394385       -0.150675 -0.279597  \n",
       "\n",
       "[1030 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df_X.columns:\n",
    "    # Calculate mean and standard deviation of each column\n",
    "    mean = df_X[column].mean()\n",
    "    std = df_X[column].std()\n",
    "    # Normalize the data\n",
    "    df_X[column] = df_X[column].apply(lambda x : (x - mean) / std)\n",
    "    \n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using the code in PART A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split helper function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model on the training data using 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 74.6526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.65264129638672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer : Acceptable loss on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 784us/step - loss: 56.5409\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 55.4725\n",
      "10/10 [==============================] - 0s 821us/step - loss: 41.0844\n",
      "10/10 [==============================] - 0s 823us/step - loss: 43.2335\n",
      "10/10 [==============================] - 0s 780us/step - loss: 37.1557\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 46.8614\n",
      "10/10 [==============================] - 0s 735us/step - loss: 45.8242\n",
      "10/10 [==============================] - 0s 925us/step - loss: 43.4680\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 44.9544\n",
      "10/10 [==============================] - 0s 881us/step - loss: 46.7172\n",
      "10/10 [==============================] - 0s 841us/step - loss: 42.6304\n",
      "10/10 [==============================] - 0s 821us/step - loss: 52.2791\n",
      "10/10 [==============================] - 0s 790us/step - loss: 43.6291\n",
      "10/10 [==============================] - 0s 743us/step - loss: 50.2634\n",
      "10/10 [==============================] - 0s 741us/step - loss: 42.7328\n",
      "10/10 [==============================] - 0s 949us/step - loss: 36.7563\n",
      "10/10 [==============================] - 0s 831us/step - loss: 44.7560\n",
      "10/10 [==============================] - 0s 729us/step - loss: 37.1278\n",
      "10/10 [==============================] - 0s 854us/step - loss: 42.6501\n",
      "10/10 [==============================] - 0s 699us/step - loss: 42.8013\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 36.8219\n",
      "10/10 [==============================] - 0s 725us/step - loss: 48.9743\n",
      "10/10 [==============================] - 0s 731us/step - loss: 43.7139\n",
      "10/10 [==============================] - 0s 798us/step - loss: 39.5333\n",
      "10/10 [==============================] - 0s 715us/step - loss: 37.0658\n",
      "10/10 [==============================] - 0s 768us/step - loss: 38.7370\n",
      "10/10 [==============================] - 0s 791us/step - loss: 44.9333\n",
      "10/10 [==============================] - 0s 719us/step - loss: 42.2270\n",
      "10/10 [==============================] - 0s 715us/step - loss: 43.2226\n",
      "10/10 [==============================] - 0s 783us/step - loss: 43.9019\n",
      "10/10 [==============================] - 0s 817us/step - loss: 42.0429\n",
      "10/10 [==============================] - 0s 738us/step - loss: 44.5321\n",
      "10/10 [==============================] - 0s 780us/step - loss: 39.9932\n",
      "10/10 [==============================] - 0s 790us/step - loss: 37.9780\n",
      "10/10 [==============================] - 0s 786us/step - loss: 35.1350\n",
      "10/10 [==============================] - 0s 887us/step - loss: 39.4550\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 42.6972\n",
      "10/10 [==============================] - 0s 738us/step - loss: 47.4185\n",
      "10/10 [==============================] - 0s 881us/step - loss: 37.0897\n",
      "10/10 [==============================] - 0s 755us/step - loss: 39.7756\n",
      "10/10 [==============================] - 0s 930us/step - loss: 45.3756\n",
      "10/10 [==============================] - 0s 808us/step - loss: 45.3053\n",
      "10/10 [==============================] - 0s 688us/step - loss: 40.9073\n",
      "10/10 [==============================] - 0s 828us/step - loss: 43.1269\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 35.9139\n",
      "10/10 [==============================] - 0s 833us/step - loss: 38.9265\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 36.6036\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 41.5733\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 40.5066\n",
      "10/10 [==============================] - 0s 775us/step - loss: 38.0079\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_list =  [] # List to hold all the mean_squared_erros\n",
    "\n",
    "for step_number in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3)\n",
    "    history = model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    step_mean_squared_error = model.evaluate(X_test, y_test)\n",
    "    mean_squared_error_list.append(step_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared errors is :  42.568669662475585\n",
      "Standard Deviation of mean squared errors is :  22.050604691646644\n"
     ]
    }
   ],
   "source": [
    "mean_of_mse = sum(mean_squared_error_list) / len(mean_squared_error_list)\n",
    "standard_deviation_of_mse = sum([((x - mean_of_mse) ** 2) for x in mean_squared_error_list]) / len(mean_squared_error_list)\n",
    "\n",
    "print(\"Mean of mean squared errors is : \",mean_of_mse)\n",
    "print(\"Standard Deviation of mean squared errors is : \", standard_deviation_of_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
