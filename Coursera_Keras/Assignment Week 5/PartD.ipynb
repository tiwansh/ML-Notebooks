{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"concrete_data.csv\")\n",
    "df_y = df[\"Strength\"]\n",
    "df_X = df.drop([\"Strength\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D. Increase the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a single hidden layer of 10 nodes, and a ReLU activation function\n",
    "model.add(Dense(10, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))  # Output layer for linear regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using Adam optimizer and loss mean_squared_error\n",
    "model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column, calculate mean and standard deviation and then for each value, subtract mean and divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>-0.045623</td>\n",
       "      <td>0.487998</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>-0.092126</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>-1.322363</td>\n",
       "      <td>-0.065861</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.392628</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.959602</td>\n",
       "      <td>0.675872</td>\n",
       "      <td>0.702285</td>\n",
       "      <td>-1.993711</td>\n",
       "      <td>0.496651</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>-1.269472</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.850222</td>\n",
       "      <td>0.521336</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-1.035561</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-1.168042</td>\n",
       "      <td>1.307430</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>0.214537</td>\n",
       "      <td>0.191074</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-0.193939</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.376762</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.400971</td>\n",
       "      <td>-1.394385</td>\n",
       "      <td>-0.150675</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4    -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "...        ...                 ...       ...       ...               ...   \n",
       "1025 -0.045623            0.487998  0.564271 -0.092126          0.451190   \n",
       "1026  0.392628           -0.856472  0.959602  0.675872          0.702285   \n",
       "1027 -1.269472            0.759210  0.850222  0.521336         -0.017520   \n",
       "1028 -1.168042            1.307430 -0.846733 -0.279443          0.852942   \n",
       "1029 -0.193939            0.308349  0.376762  0.891286          0.400971   \n",
       "\n",
       "      Coarse Aggregate  Fine Aggregate       Age  \n",
       "0             0.862735       -1.217079 -0.279597  \n",
       "1             1.055651       -1.217079 -0.279597  \n",
       "2            -0.526262       -2.239829  3.551340  \n",
       "3            -0.526262       -2.239829  5.055221  \n",
       "4             0.070492        0.647569  4.976069  \n",
       "...                ...             ...       ...  \n",
       "1025         -1.322363       -0.065861 -0.279597  \n",
       "1026         -1.993711        0.496651 -0.279597  \n",
       "1027         -1.035561        0.080068 -0.279597  \n",
       "1028          0.214537        0.191074 -0.279597  \n",
       "1029         -1.394385       -0.150675 -0.279597  \n",
       "\n",
       "[1030 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df_X.columns:\n",
    "    # Calculate mean and standard deviation of each column\n",
    "    mean = df_X[column].mean()\n",
    "    std = df_X[column].std()\n",
    "    # Normalize the data\n",
    "    df_X[column] = df_X[column].apply(lambda x : (x - mean) / std)\n",
    "    \n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using the code in PART A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_split helper function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model on the training data using 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1564.1207\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1136.2260\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 324.4275\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 200.5843\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 179.1151\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 158.1520\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 141.9976\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 123.0781\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 123.6399\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 127.0844\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 108.5803\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 110.5946\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 98.0200\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 100.9517\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 94.9258\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 95.4014\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 88.2811\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 84.3528\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 87.7745\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 84.6603\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 82.2133\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 73.0241\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 79.7891\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 70.6290\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 71.7017\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 74.9688\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 73.9737\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 79.5884\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 68.5397: 0s - loss: 68.15\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.7690\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 80.0549\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 75.4603\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 72.6251\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.9386\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 72.9603\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 73.2064\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 73.3503\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 74.3476\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.2017\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 76.4846\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 72.5292\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 74.4985\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.6698\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.5290\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 74.0389\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 71.6908\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 70.2163\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.1197\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 61.1644\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.9838\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 58.9742\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.8627\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.6501\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3914\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.0961\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5184\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1609\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2184\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.7931\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.7154\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.5752\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9405\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2674\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1907\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.9530\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3867\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.9854\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.8364\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5351\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 39.3515\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.9962\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 35.9997\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.8922\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7018\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.1911\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.4416\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.9299\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.7750\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.8534\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.8546\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.7764\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.9105\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 39.0778\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.3365\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.2722\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.6452\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 35.5466\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.1461\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.8363\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.3672\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.9900\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 38.1230\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.6194\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.3516\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.0941\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.7104\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3552\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.9449\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6491\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.0276\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 37.5969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.596946716308594"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer : Acceptable loss on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 946us/step - loss: 43.4134\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 41.6182\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 39.7729\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29.8601\n",
      "10/10 [==============================] - 0s 806us/step - loss: 27.9243\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 32.9191\n",
      "10/10 [==============================] - 0s 877us/step - loss: 31.2642\n",
      "10/10 [==============================] - 0s 952us/step - loss: 26.9854\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 27.2401\n",
      "10/10 [==============================] - 0s 930us/step - loss: 25.1422\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 24.0941\n",
      "10/10 [==============================] - 0s 807us/step - loss: 21.5688\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 31.0105\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.1990\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 24.8513\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.2245\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.8886\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 27.7794\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 26.7239\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 22.3945\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.4418\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.8335\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.8456\n",
      "10/10 [==============================] - 0s 870us/step - loss: 22.9876\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 20.5777\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 21.9531\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.0201\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 22.9593\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.4029\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.5754\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.0690\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 17.8878\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 21.1235\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 19.2363\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 19.2117\n",
      "10/10 [==============================] - 0s 841us/step - loss: 16.5381\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 16.9557\n",
      "10/10 [==============================] - 0s 967us/step - loss: 20.8912\n",
      "10/10 [==============================] - 0s 969us/step - loss: 25.5471\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.0398\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.7338\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 18.2888\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.0782\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.3211\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 16.6612\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 15.7453\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 16.5892\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 17.4830\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.3369\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 18.8035\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error_list =  [] # List to hold all the mean_squared_erros\n",
    "\n",
    "for step_number in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3)\n",
    "    history = model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    step_mean_squared_error = model.evaluate(X_test, y_test)\n",
    "    mean_squared_error_list.append(step_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared errors is :  23.58025568008423\n",
      "Standard Deviation of mean squared errors is :  39.92122724577492\n"
     ]
    }
   ],
   "source": [
    "mean_of_mse = sum(mean_squared_error_list) / len(mean_squared_error_list)\n",
    "standard_deviation_of_mse = sum([((x - mean_of_mse) ** 2) for x in mean_squared_error_list]) / len(mean_squared_error_list)\n",
    "\n",
    "print(\"Mean of mean squared errors is : \",mean_of_mse)\n",
    "print(\"Standard Deviation of mean squared errors is : \", standard_deviation_of_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
